{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Amrita Sinha Roy \n",
    "Student ID: 501306770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51KDlwmbLBc8",
    "outputId": "58b9f97e-0653-48a8-fb0d-d5d9f9fb7893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XffPn7QsgrZR",
    "outputId": "f86cdb4a-acbf-43f0-9619-9a695c945c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOORyg6KLEAJ",
    "outputId": "e8daadb2-d3da-40d0-8b85-792f6b0b3e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/wce_endoscopy_dataset\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/wce_endoscopy_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhyBeXsWLV6C",
    "outputId": "37780556-1a46-4885-d121-bb5f69b7f54c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  archive.zip\n",
      "replace test/0_normal/test_normal_ (1).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ73EYayKw4Y"
   },
   "source": [
    "Efficient Net B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMG9R39JJQk5",
    "outputId": "de5913ea-cf59-43b4-ab01-28f1c0da78bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "\u001b[1m31790344/31790344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 767ms/step - accuracy: 0.2367 - loss: 1.4189 - val_accuracy: 0.2500 - val_loss: 1.3863\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 537ms/step - accuracy: 0.2446 - loss: 1.3870 - val_accuracy: 0.2500 - val_loss: 1.3863\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 545ms/step - accuracy: 0.2480 - loss: 1.3864 - val_accuracy: 0.2500 - val_loss: 1.3863\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 542ms/step - accuracy: 0.2391 - loss: 1.3863 - val_accuracy: 0.2500 - val_loss: 1.3864\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 542ms/step - accuracy: 0.2538 - loss: 1.3867 - val_accuracy: 0.2500 - val_loss: 1.3863\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 367ms/step - accuracy: 0.2497 - loss: 1.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB2 Test Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Set directories for training, validation, and test sets\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Data preparation\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# EfficientNetB2 Model\n",
    "def build_efficientnetb2(input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = EfficientNetB2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze base model weights\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnetb2()\n",
    "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training EfficientNetB2\n",
    "epochs = 5\n",
    "history = efficientnet_model.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = efficientnet_model.evaluate(test_generator)\n",
    "print(f\"EfficientNetB2 Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "efficientnet_model.save(\"efficientnetb2_endoscopy.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlqHrsZyxmMu"
   },
   "source": [
    "Added mean and std of ImageNet traning data for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoXA7Z3fotdZ",
    "outputId": "c4a5d6c3-1de3-4cca-c474-887b6769877f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 737ms/step - accuracy: 0.3237 - loss: 1.3800 - val_accuracy: 0.4425 - val_loss: 1.2637\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 616ms/step - accuracy: 0.3882 - loss: 1.2875 - val_accuracy: 0.4975 - val_loss: 1.1938\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 617ms/step - accuracy: 0.4303 - loss: 1.2343 - val_accuracy: 0.4565 - val_loss: 1.1411\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 614ms/step - accuracy: 0.4557 - loss: 1.2083 - val_accuracy: 0.5085 - val_loss: 1.1785\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 616ms/step - accuracy: 0.4552 - loss: 1.1763 - val_accuracy: 0.4585 - val_loss: 1.2181\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 386ms/step - accuracy: 0.4784 - loss: 1.1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB2 Test Accuracy: 0.4662500023841858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Set directories for training, validation, and test sets\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Mean and standard deviation values for ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Custom preprocessing function to normalize the image\n",
    "def preprocess_image(img):\n",
    "    # Normalize image (scale it to [0, 1], then use ImageNet mean and std)\n",
    "    img = img / 255.0  # First normalize to [0, 1]\n",
    "    img = (img - mean) / std  # Then apply ImageNet normalization\n",
    "    return img\n",
    "\n",
    "# Data preparation\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Applying custom preprocessing using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# EfficientNetB2 Model\n",
    "def build_efficientnetb2(input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = EfficientNetB2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze base model weights\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnetb2()\n",
    "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training EfficientNetB2\n",
    "epochs = 5\n",
    "history = efficientnet_model.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = efficientnet_model.evaluate(test_generator)\n",
    "print(f\"EfficientNetB2 Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "efficientnet_model.save(\"efficientnetb2_endoscopy.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OchniUQQxts3"
   },
   "source": [
    "Added earlystopping, reducelr on plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkS867A5twf4",
    "outputId": "fe8cbc09-f8d9-4004-a55d-ff11bd323d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.3080 - loss: 1.3744\n",
      "Epoch 1: val_loss improved from inf to 1.22696, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 739ms/step - accuracy: 0.3084 - loss: 1.3741 - val_accuracy: 0.5055 - val_loss: 1.2270 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.3924 - loss: 1.2771\n",
      "Epoch 2: val_loss improved from 1.22696 to 1.16230, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 633ms/step - accuracy: 0.3925 - loss: 1.2769 - val_accuracy: 0.4740 - val_loss: 1.1623 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.4429 - loss: 1.2152\n",
      "Epoch 3: val_loss improved from 1.16230 to 1.13052, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 666ms/step - accuracy: 0.4430 - loss: 1.2149 - val_accuracy: 0.5165 - val_loss: 1.1305 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - accuracy: 0.4682 - loss: 1.1735\n",
      "Epoch 4: val_loss did not improve from 1.13052\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 869ms/step - accuracy: 0.4682 - loss: 1.1733 - val_accuracy: 0.4865 - val_loss: 1.1487 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.4549 - loss: 1.1827\n",
      "Epoch 5: val_loss improved from 1.13052 to 1.11320, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 790ms/step - accuracy: 0.4550 - loss: 1.1825 - val_accuracy: 0.5230 - val_loss: 1.1132 - learning_rate: 0.0010\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 385ms/step - accuracy: 0.5338 - loss: 1.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB2 Test Accuracy: 0.5412499904632568\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Set directories for training, validation, and test sets\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Mean and standard deviation values for ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Custom preprocessing function to normalize the image\n",
    "def preprocess_image(img):\n",
    "    # Normalize image (scale it to [0, 1], then use ImageNet mean and std)\n",
    "    img = img / 255.0  # First normalize to [0, 1]\n",
    "    img = (img - mean) / std  # Then apply ImageNet normalization\n",
    "    return img\n",
    "\n",
    "# Data preparation\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Applying custom preprocessing using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# EfficientNetB2 Model\n",
    "def build_efficientnetb2(input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = EfficientNetB2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze base model weights\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnetb2()\n",
    "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau & ModelCheckpoint & EarlyStopping callbacks\n",
    "checkpoint_path = 'model/Model.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "anne = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min')\n",
    "\n",
    "# Training EfficientNetB2 with callbacks\n",
    "epochs = 5\n",
    "history = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[anne, checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = efficientnet_model.evaluate(test_generator)\n",
    "print(f\"EfficientNetB2 Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "efficientnet_model.save(\"efficientnetb2_endoscopy.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktYFoJN9wT_-",
    "outputId": "d15d090f-82b5-4822-cd91-971f9f983876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - accuracy: 0.3846 - loss: 1.4974\n",
      "Epoch 1: val_loss improved from inf to 1.27634, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 849ms/step - accuracy: 0.3849 - loss: 1.4964 - val_accuracy: 0.4370 - val_loss: 1.2763 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4929 - loss: 1.1976\n",
      "Epoch 2: val_loss improved from 1.27634 to 1.17216, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 665ms/step - accuracy: 0.4929 - loss: 1.1974 - val_accuracy: 0.3890 - val_loss: 1.1722 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.5132 - loss: 1.1434\n",
      "Epoch 3: val_loss did not improve from 1.17216\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 646ms/step - accuracy: 0.5132 - loss: 1.1433 - val_accuracy: 0.3175 - val_loss: 1.2371 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.5389 - loss: 1.1005\n",
      "Epoch 4: val_loss improved from 1.17216 to 1.05675, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 794ms/step - accuracy: 0.5390 - loss: 1.1003 - val_accuracy: 0.5745 - val_loss: 1.0568 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.5419 - loss: 1.0378\n",
      "Epoch 5: val_loss did not improve from 1.05675\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 628ms/step - accuracy: 0.5419 - loss: 1.0379 - val_accuracy: 0.4965 - val_loss: 1.3755 - learning_rate: 0.0010\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 414ms/step - accuracy: 0.4917 - loss: 1.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB2 Test Accuracy: 0.48875001072883606\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Set directories for training, validation, and test sets\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Mean and standard deviation values for ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Custom preprocessing function to normalize the image\n",
    "def preprocess_image(img):\n",
    "    # Normalize image (scale it to [0, 1], then use ImageNet mean and std)\n",
    "    img = img / 255.0  # First normalize to [0, 1]\n",
    "    img = (img - mean) / std  # Then apply ImageNet normalization\n",
    "    return img\n",
    "\n",
    "# Data preparation\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Applying custom preprocessing using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# EfficientNetB2 Model\n",
    "def build_efficientnetb2(input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = EfficientNetB2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze base model weights\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        # layers.GlobalAveragePooling2D(),\n",
    "        # layers.Dense(128, activation='relu'),\n",
    "        # layers.Dropout(0.5),\n",
    "        # layers.Dense(num_classes, activation='softmax')\n",
    "        layers.GaussianNoise(0.35),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256,activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GaussianNoise(0.35),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnetb2()\n",
    "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau & ModelCheckpoint & EarlyStopping callbacks\n",
    "checkpoint_path = 'model/Model.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "anne = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min')\n",
    "\n",
    "# Training EfficientNetB2 with callbacks\n",
    "epochs = 5\n",
    "history = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[anne, checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = efficientnet_model.evaluate(test_generator)\n",
    "print(f\"EfficientNetB2 Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "efficientnet_model.save(\"efficientnetb2_endoscopy.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLffdKxDyVjn",
    "outputId": "3aa4e68c-6235-46d2-c122-8486b898bd30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.2854 - loss: 1.3929\n",
      "Epoch 1: val_loss improved from inf to 1.26763, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 761ms/step - accuracy: 0.2858 - loss: 1.3925 - val_accuracy: 0.3985 - val_loss: 1.2676 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step - accuracy: 0.3777 - loss: 1.2833\n",
      "Epoch 2: val_loss improved from 1.26763 to 1.21894, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 638ms/step - accuracy: 0.3780 - loss: 1.2831 - val_accuracy: 0.4410 - val_loss: 1.2189 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.4324 - loss: 1.2320\n",
      "Epoch 3: val_loss improved from 1.21894 to 1.18919, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 645ms/step - accuracy: 0.4325 - loss: 1.2319 - val_accuracy: 0.4800 - val_loss: 1.1892 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step - accuracy: 0.4424 - loss: 1.2021\n",
      "Epoch 4: val_loss improved from 1.18919 to 1.14997, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 639ms/step - accuracy: 0.4425 - loss: 1.2020 - val_accuracy: 0.5120 - val_loss: 1.1500 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 0.4654 - loss: 1.1755\n",
      "Epoch 5: val_loss improved from 1.14997 to 1.14047, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 667ms/step - accuracy: 0.4654 - loss: 1.1755 - val_accuracy: 0.5115 - val_loss: 1.1405 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.4828 - loss: 1.1412\n",
      "Epoch 6: val_loss did not improve from 1.14047\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 642ms/step - accuracy: 0.4827 - loss: 1.1412 - val_accuracy: 0.4900 - val_loss: 1.2098 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - accuracy: 0.4815 - loss: 1.1447\n",
      "Epoch 7: val_loss improved from 1.14047 to 1.09913, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 817ms/step - accuracy: 0.4815 - loss: 1.1447 - val_accuracy: 0.5310 - val_loss: 1.0991 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.5108 - loss: 1.1250\n",
      "Epoch 8: val_loss did not improve from 1.09913\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 654ms/step - accuracy: 0.5108 - loss: 1.1250 - val_accuracy: 0.5275 - val_loss: 1.1598 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.5182 - loss: 1.0969\n",
      "Epoch 9: val_loss did not improve from 1.09913\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 818ms/step - accuracy: 0.5181 - loss: 1.0969 - val_accuracy: 0.4825 - val_loss: 1.1736 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.4812 - loss: 1.1321\n",
      "Epoch 10: val_loss did not improve from 1.09913\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 781ms/step - accuracy: 0.4813 - loss: 1.1319 - val_accuracy: 0.5365 - val_loss: 1.1346 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - accuracy: 0.5156 - loss: 1.0686\n",
      "Epoch 11: val_loss improved from 1.09913 to 1.09736, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 812ms/step - accuracy: 0.5157 - loss: 1.0687 - val_accuracy: 0.5395 - val_loss: 1.0974 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.5345 - loss: 1.0677\n",
      "Epoch 12: val_loss improved from 1.09736 to 1.08925, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 631ms/step - accuracy: 0.5346 - loss: 1.0676 - val_accuracy: 0.5270 - val_loss: 1.0893 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.5511 - loss: 1.0479\n",
      "Epoch 13: val_loss did not improve from 1.08925\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 629ms/step - accuracy: 0.5509 - loss: 1.0479 - val_accuracy: 0.5520 - val_loss: 1.1280 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.5473 - loss: 1.0264\n",
      "Epoch 14: val_loss improved from 1.08925 to 1.08066, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 644ms/step - accuracy: 0.5473 - loss: 1.0265 - val_accuracy: 0.5640 - val_loss: 1.0807 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371ms/step - accuracy: 0.5601 - loss: 1.0067\n",
      "Epoch 15: val_loss did not improve from 1.08066\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 785ms/step - accuracy: 0.5600 - loss: 1.0068 - val_accuracy: 0.5380 - val_loss: 1.1522 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.5483 - loss: 1.0365\n",
      "Epoch 16: val_loss did not improve from 1.08066\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 606ms/step - accuracy: 0.5484 - loss: 1.0363 - val_accuracy: 0.5545 - val_loss: 1.1047 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 0.5539 - loss: 1.0081\n",
      "Epoch 17: val_loss improved from 1.08066 to 1.01818, saving model to model/Model.keras\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 627ms/step - accuracy: 0.5539 - loss: 1.0081 - val_accuracy: 0.5985 - val_loss: 1.0182 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.5688 - loss: 1.0064\n",
      "Epoch 18: val_loss did not improve from 1.01818\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 637ms/step - accuracy: 0.5688 - loss: 1.0062 - val_accuracy: 0.5530 - val_loss: 1.1351 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 0.5785 - loss: 0.9751\n",
      "Epoch 19: val_loss did not improve from 1.01818\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 617ms/step - accuracy: 0.5784 - loss: 0.9750 - val_accuracy: 0.5945 - val_loss: 1.0183 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.5802 - loss: 0.9510\n",
      "Epoch 20: val_loss did not improve from 1.01818\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 602ms/step - accuracy: 0.5801 - loss: 0.9512 - val_accuracy: 0.5560 - val_loss: 1.0302 - learning_rate: 0.0010\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 409ms/step - accuracy: 0.5798 - loss: 0.9535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB2 Test Accuracy: 0.5637500286102295\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Set directories for training, validation, and test sets\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'test'\n",
    "\n",
    "# Mean and standard deviation values for ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Custom preprocessing function to normalize the image\n",
    "def preprocess_image(img):\n",
    "    # Normalize image (scale it to [0, 1], then use ImageNet mean and std)\n",
    "    img = img / 255.0  # First normalize to [0, 1]\n",
    "    img = (img - mean) / std  # Then apply ImageNet normalization\n",
    "    return img\n",
    "\n",
    "# Data preparation\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Applying custom preprocessing using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# EfficientNetB2 Model\n",
    "def build_efficientnetb2(input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model = EfficientNetB2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze base model weights\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnetb2()\n",
    "efficientnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau & ModelCheckpoint & EarlyStopping callbacks\n",
    "checkpoint_path = 'model/Model.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "anne = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min')\n",
    "\n",
    "# Training EfficientNetB2 with callbacks\n",
    "epochs = 20\n",
    "history = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[anne, checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = efficientnet_model.evaluate(test_generator)\n",
    "print(f\"EfficientNetB2 Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save model\n",
    "efficientnet_model.save(\"efficientnetb2_endoscopy.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
